# INGRES AI Chatbot Evaluation System

This comprehensive evaluation system tests your INGRES AI chatbot using industry-standard metrics: **BERT scores** for semantic similarity and **BLEU scores** for lexical overlap.

## ğŸš€ Quick Start

### Option 1: Run from Backend Directory
```bash
cd backend
python run_evaluation.py
```

### Option 2: Run Directly
```bash
cd backend/tests
python evaluate_chatbot.py
```

## ğŸ“Š What Gets Evaluated

The system evaluates your chatbot on 10 carefully crafted test cases covering:

- **Connection issues** - Database connectivity problems
- **Configuration** - Port settings and server configuration  
- **Troubleshooting** - Error diagnosis and resolution
- **Transactions** - Isolation levels and transaction management
- **Performance** - Query optimization and performance tuning
- **Error Handling** - Specific error codes and solutions
- **Backup** - Database backup procedures
- **Data Types** - INGRES data type usage
- **Monitoring** - Performance monitoring techniques
- **Maintenance** - Database upgrade procedures

## ğŸ¯ Evaluation Metrics

### BERT Scores (Semantic Similarity)
- **Precision**: How relevant the generated response is
- **Recall**: How much of the expected content is covered
- **F1 Score**: Overall semantic similarity (0-1 scale)

### BLEU Scores (Lexical Overlap)
- Measures exact word/phrase matching with expected answers
- Scale: 0-1 (higher is better)

## ğŸ“‹ Output Files

After evaluation, you'll find these files in `evaluation_results/`:

1. **`evaluation_report.txt`** - Human-readable summary with performance assessment
2. **`detailed_results.json`** - Complete results data for analysis
3. **`evaluation_results.csv`** - Spreadsheet format for further analysis
4. **`evaluation_charts.png`** - Visual performance charts

## ğŸ† Performance Interpretation

### BERT F1 Scores
- **0.85+**: ğŸŸ¢ EXCELLENT - Very high semantic similarity
- **0.75-0.84**: ğŸŸ¡ GOOD - Good similarity with room for improvement  
- **0.65-0.74**: ğŸŸ  FAIR - Moderate similarity, needs improvement
- **<0.65**: ğŸ”´ POOR - Low similarity, significant improvement needed

### BLEU Scores
- **0.40+**: ğŸŸ¢ EXCELLENT - High lexical overlap
- **0.25-0.39**: ğŸŸ¡ GOOD - Reasonable lexical overlap
- **0.15-0.24**: ğŸŸ  FAIR - Some overlap, could be better
- **<0.15**: ğŸ”´ POOR - Low overlap, needs improvement

## ğŸ”§ Key Features

### Clean Response Processing
- Automatically removes chunk indicators and technical metadata
- Eliminates internal processing information
- Formats responses as coherent single paragraphs
- No "Based on documentation..." prefixes or section numbers

### Comprehensive Analysis
- Category-wise performance breakdown
- Statistical analysis with means and standard deviations
- Best/worst performing example comparisons
- Visual charts and correlation analysis

### Professional Reporting
- Clean, emoji-enhanced reports
- Multiple output formats (TXT, JSON, CSV, PNG)
- Performance assessment with actionable insights

## ğŸ“ File Structure

```
backend/tests/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ evaluation_dataset.json   # Test questions and expected answers
â”œâ”€â”€ response_formatter.py     # Clean response processing
â”œâ”€â”€ evaluate_chatbot.py       # Main evaluation script
â””â”€â”€ evaluation_results/       # Generated results directory
    â”œâ”€â”€ evaluation_report.txt
    â”œâ”€â”€ detailed_results.json
    â”œâ”€â”€ evaluation_results.csv
    â””â”€â”€ evaluation_charts.png
```

## ğŸ› ï¸ Dependencies

The evaluation system automatically installs these required packages:
- `bert-score` - For BERT similarity scoring
- `nltk` - Natural language processing
- `sacrebleu` - BLEU score calculation
- `pandas` - Data analysis
- `matplotlib`, `seaborn` - Visualizations
- `evaluate`, `datasets` - Hugging Face evaluation tools

## ğŸ¨ Sample Output

```
ğŸ” INGRES AI Chatbot Evaluation System
==================================================
ğŸ”§ Initializing evaluation components...
ğŸ“Š Loaded 10 test cases
ğŸš€ Starting comprehensive evaluation...

1ï¸âƒ£ Generating chatbot responses...
2ï¸âƒ£ Calculating BERT scores...
3ï¸âƒ£ Calculating BLEU scores...
4ï¸âƒ£ Analyzing results by category...
5ï¸âƒ£ Computing overall statistics...
6ï¸âƒ£ Generating evaluation report...

==================================================
ğŸ“‹ EVALUATION SUMMARY
==================================================
Total Tests: 10
BERT F1 Score: 0.7842
BLEU Score: 0.2156
==================================================
ğŸ“ Results saved to: /path/to/evaluation_results
```

## ğŸ’¡ Tips for Better Scores

1. **For Higher BERT Scores**: Focus on semantic accuracy and comprehensive coverage of the topic
2. **For Higher BLEU Scores**: Use similar terminology and phrasing as expected answers
3. **Clean Responses**: The system automatically removes chunks and technical info for fair evaluation
4. **Consistent Formatting**: Responses are normalized to single paragraphs for consistent scoring

---

**Ready to evaluate your chatbot?** Run `python run_evaluation.py` from the backend directory!