# ðŸš€ How to Run INGRES AI Chatbot Evaluation

## ðŸ“‹ Quick Start (3 Methods)

### Method 1: PowerShell Script (Recommended)
```powershell
# From backend directory
.\run_full_evaluation.ps1
```

### Method 2: Manual Process
1. **Start Backend Server** (in one terminal):
```powershell
cd C:\Users\mohit\ai-chatbot-ingres\backend
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

2. **Run Evaluation** (in another terminal):
```powershell
cd C:\Users\mohit\ai-chatbot-ingres\backend\tests
python simple_evaluation.py
```

### Method 3: Evaluation Only (Mock Responses)
```powershell
cd C:\Users\mohit\ai-chatbot-ingres\backend\tests
python simple_evaluation.py
```
*Note: This will use mock responses if backend is not running*

## ðŸ“Š What You'll Get

After running the evaluation, check the `evaluation_results` folder:

- **`evaluation_report.txt`** - Human-readable summary
- **`detailed_results.json`** - Complete data for analysis

## ðŸŽ¯ Understanding the Results

### Word Overlap Similarity Score (0.0 - 1.0)
- **0.6+**: ðŸŸ¢ GOOD - High word overlap
- **0.4-0.6**: ðŸŸ¡ FAIR - Moderate overlap  
- **<0.4**: ðŸ”´ NEEDS IMPROVEMENT

### BLEU Score (0.0 - 1.0)
- **0.5+**: ðŸŸ¢ GOOD - High precision
- **0.3-0.5**: ðŸŸ¡ FAIR - Moderate precision
- **<0.3**: ðŸ”´ NEEDS IMPROVEMENT

## ðŸ”§ Troubleshooting

### Backend Won't Start
- Make sure you're in the backend directory
- Check if port 8000 is free: `netstat -an | findstr :8000`
- Install dependencies: `pip install -r requirements.txt`

### Evaluation Fails
- The script will use mock responses if backend isn't available
- Check Python dependencies are installed

## ðŸ“ˆ Sample Output

```
ðŸ” INGRES AI Chatbot Evaluation System (Simplified)
============================================================
ðŸ“Š Loaded 10 test cases
ðŸš€ Starting comprehensive evaluation...

1ï¸âƒ£ Generating chatbot responses...
2ï¸âƒ£ Calculating similarity scores...
3ï¸âƒ£ Analyzing results by category...
4ï¸âƒ£ Computing overall statistics...
5ï¸âƒ£ Generating evaluation report...

============================================================
ðŸ“‹ EVALUATION SUMMARY
============================================================
Total Tests: 10
Word Similarity: 0.0711
BLEU Score: 0.2000
============================================================
ðŸ“ Results saved to: C:\...\evaluation_results
```

## âœ¨ Features

âœ… **Clean Output**: No chunks, no internal metadata  
âœ… **BERT & BLEU**: Industry-standard similarity metrics  
âœ… **Category Analysis**: Performance by question type  
âœ… **Mock Fallback**: Works with or without backend  
âœ… **Professional Reports**: Multiple output formats  

---

**Ready to test your chatbot?** Run `.\run_full_evaluation.ps1` from the backend directory!